{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODULE IMPORTATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import inflect\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import punkt\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "##### UNCOMMENT THIS SECTION IF FIRT TIME RUNNING\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "#####\n",
    "\n",
    "# Set seed for random results base calculation\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dataframe des avis\n",
      "Size : (10000, 1)\n",
      "Head of imported dataset :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis\n",
       "0  Obviously made to show famous 1950s stripper M...\n",
       "1  This film was more effective in persuading me ...\n",
       "2  Unless you are already familiar with the pop s...\n",
       "3  From around the time Europe began fighting Wor...\n",
       "4  Im not surprised that even cowgirls get the bl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dataframe des scores\n",
      "Size : (10000, 1)\n",
      "Head of imported dataset :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score\n",
       "0     -1\n",
       "1     -1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : (10000, 2)\n",
      "Head of merged dataset :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  Obviously made to show famous 1950s stripper M...     -1\n",
       "1  This film was more effective in persuading me ...     -1\n",
       "2  Unless you are already familiar with the pop s...     -1\n",
       "3  From around the time Europe began fighting Wor...     -1\n",
       "4  Im not surprised that even cowgirls get the bl...     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of shuffled dataset :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After having read two or three negative review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I recently (May 2008) discovered that this chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pathetic is the word. Bad acting, pathetic scr...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spencer Tracy and Katherine Hepburn would roll...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This in my opinion is one of the best action m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  After having read two or three negative review...      1\n",
       "1  I recently (May 2008) discovered that this chi...      1\n",
       "2  Pathetic is the word. Bad acting, pathetic scr...     -1\n",
       "3  Spencer Tracy and Katherine Hepburn would roll...     -1\n",
       "4  This in my opinion is one of the best action m...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DEFINITION OF SOME OF THE RESOURCES USED THROUGHOUT THIS NOTEBOOK\n",
    "DATA_PATH = 'Dataset/dataset.csv'\n",
    "TARGET_PATH = 'Dataset/labels.csv'\n",
    "PIPELINE_PATH = 'pipeline.pkl'\n",
    "\n",
    "#DEFINITION OF IMPORTATION, MERGING AND SHUFFLING FUNCTIONS OF DATASETS\n",
    "def import_dataset(dataset_path, importation_message=None, sep='\\t', names=None):\n",
    "    \"\"\"\n",
    "    imports a dataset from a given path\n",
    "    returns the dataframe containing the imported dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n{}\".format(importation_message))\n",
    "    df = pd.read_csv(dataset_path, sep=sep, header=None, names=names, encoding='utf-8')\n",
    "    print('Size : {}'.format(df.shape))\n",
    "    print('Head of imported dataset :')\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def merge_datasets(df1, df2):\n",
    "    \"\"\"\n",
    "    merges datasets contained within dataframes df1 and df2\n",
    "    returns a new dataframe containing the merged datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df1.join(df2)\n",
    "    \n",
    "    print('Size : {}'.format(df.shape))\n",
    "    print('Head of merged dataset :')\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def shuffle_dataset(df):\n",
    "    \"\"\"\n",
    "    shuffles dataset entries and reset indexes\n",
    "    returns a new dataframe containing the shuffled dataset with the reset indexes\n",
    "    \"\"\"\n",
    "    shuffled_df = shuffle(df)\n",
    "    shuffled_df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    print('Head of shuffled dataset :')\n",
    "    display(shuffled_df.head())\n",
    "    \n",
    "    return shuffled_df\n",
    "\n",
    "#Importation of opinion dataset\n",
    "df_avis = import_dataset(DATA_PATH, importation_message=\"\\nDataframe des avis\", sep='\\t', names=['Avis'])\n",
    "\n",
    "#Importation of scores dataset\n",
    "df_score = import_dataset(TARGET_PATH, importation_message='\\nDataframe des scores', sep='\\t', names=['Score'])\n",
    "\n",
    "#Merging of both datasets\n",
    "df = merge_datasets(df_avis, df_score)\n",
    "\n",
    "#Shuffling of the merged dataset\n",
    "df2 = shuffle_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINITION OF SOME OF THE CONSTANTS USED THROUGHOUT THIS NOTEBOOK\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "STOP_WORDS_EXCEPTIONS = set(('not',))\n",
    "\n",
    "#POS-TAG dictionary that will be used during the lemmatization process\n",
    "POS_TAG_MAP = defaultdict(lambda : wn.NOUN)\n",
    "POS_TAG_MAP['J'] = wn.ADJ\n",
    "POS_TAG_MAP['V'] = wn.VERB\n",
    "POS_TAG_MAP['R'] = wn.ADV\n",
    "\n",
    "#parameters used by by the cross validation score function\n",
    "CV_SEED = 7 #seed used for random selection of partitions during cross validation\n",
    "CV_SCORING = 'accuracy'\n",
    "\n",
    "#parameters used by the training/set generator\n",
    "TTS_VALIDATION_SIZE = 0.3 #30% of dataset used for training\n",
    "TTS_TEST_SIZE = 1 - TTS_VALIDATION_SIZE #70% of dataset used for testing\n",
    "TTS_SEED = 30 #seed used for random selection of training/test sets\n",
    "\n",
    "#parameters used by the gridsearch function\n",
    "GRDSR_SCORING = 'accuracy'\n",
    "\n",
    "#\n",
    "\n",
    "#DEFINITION OF PREPROCESSING FUNCTIONS\n",
    "def replace_contractions(document):\n",
    "    \"\"\"\n",
    "    replaces contracted expressions in a document\n",
    "    \n",
    "    returns document with no contracted expressions\n",
    "    \"\"\"\n",
    "    return contractions.fix(document)\n",
    "\n",
    "def remove_urls(document):\n",
    "    \"\"\"\n",
    "    removes all urls in the document\n",
    "    \n",
    "    return a document without any urls\n",
    "    \"\"\"\n",
    "    return re.sub(r'https?://(www\\.)?[-\\w@:%.\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-\\w@:%_\\+.~#?&/=;]*)', '', document)\n",
    "\n",
    "def clean_sentences(document):\n",
    "    '''\n",
    "    cleans all sentences within a document, such that\n",
    "    the end of a sentence and the beginning of a new one is separated by a period (or many)\n",
    "    followed by a whitespace\n",
    "    This cleaning is required because upon removing punctuation,\n",
    "    some words get concatenated and create new meaningless terms\n",
    "    \n",
    "    example of a dirty document: \"This is a dirty sentence.Another dirty sentence begins\"\n",
    "    cleaned version: \"This is a cleaned sentence. Another cleaned sentence begins\"\n",
    "    \n",
    "    This pattern repeats with a sentence ending with a lowercase/uppercase letter and\n",
    "    another one beginning with a lowercase/uppercase letter\n",
    "    The beginning sentence could also end with a digit and the next sentence could begin with\n",
    "    a digit. Hence we get three different patterns:\n",
    "    word.*word\n",
    "    word.*digit\n",
    "    digit.*word\n",
    "    \n",
    "    returns a document with cleaned sentences\n",
    "    '''\n",
    "    word_word = r'([a-zA-Z]+\\.*)\\.([a-zA-Z]+)'\n",
    "    word_digit = r'([a-zA-Z]+\\.*)\\.(\\d+)'\n",
    "    digit_word = r'(\\d+\\.*)\\.([a-zA-Z]+)'\n",
    "    patterns = [\n",
    "        word_word, #word.word pattern\n",
    "        word_digit, #word.digit pattern\n",
    "        digit_word, #digit.word pattern\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, document):\n",
    "            document = re.sub(pattern, r'\\1. \\2', document)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def remove_non_ascii(tokens):\n",
    "    '''\n",
    "    normalizes the tokens\n",
    "    encodes tokens as ASCII characters from tokens\n",
    "    and decodes as utf-8\n",
    "    \n",
    "    returns a list of normalized and encoded as ascii tokens\n",
    "    '''\n",
    "    return [unicodedata.normalize('NFKD', token)\n",
    "           .encode('ascii', 'ignore')\n",
    "           .decode('utf-8', 'ignore')\n",
    "           for token in tokens]\n",
    "\n",
    "def split_on_characterset(tokens, regex):\n",
    "    '''\n",
    "    splits a token in tokens upon matching with the characterset defined by the regex\n",
    "    appends the tokens obtained from splitting the token to the tokens list\n",
    "    \n",
    "    returns a list of all tokens obtained after splitting problematic tokens\n",
    "    '''\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search(regex, token) :\n",
    "            new_tokens += re.split(regex, token)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"returns a list of tokens in lowercase\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def replace_numbers(tokens):\n",
    "    \"\"\"\n",
    "    replaces tokens representing whole numeric values\n",
    "    by their equivalent letter values\n",
    "    \n",
    "    returns a list of transformed tokens\n",
    "    \"\"\"\n",
    "    engine = inflect.engine()\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = token\n",
    "        if token.isdigit():\n",
    "            new_token = engine.number_to_words(token)\n",
    "        new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    \"\"\"\n",
    "    removes tokens not in \\w and \\s classes of characters.\n",
    "    by extension, all punctuation characters will be removed\n",
    "    \n",
    "    returns a list of tokens only in \\w and \\s\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        if new_token != '':\n",
    "            new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_stopwords(tokens, stopwords, exceptions):\n",
    "    '''\n",
    "    removes all stopwords (a set) from tokens (a list)\n",
    "    except those in exceptions (a set)\n",
    "    \n",
    "    returns a list of tokens that are not stopwords\n",
    "    '''\n",
    "    stop = stopwords - exceptions\n",
    "    return [token for token in tokens if token not in stop]\n",
    "\n",
    "def lemmatize(tokens, lemmatizer, pos_tag_map):\n",
    "    '''\n",
    "    lematizes all tokens using a lemmatizer and a POS-Tagging map\n",
    "    \n",
    "    returns the list of lemmatized tokens\n",
    "    '''\n",
    "    return [lemmatizer.lemmatize(token, pos_tag_map[tag[0]]) for token, tag in pos_tag(tokens)]\n",
    "    \n",
    "def normalize(tokens):\n",
    "    '''\n",
    "    normalizes all the tokens by using all preprocessing\n",
    "    functions taking a list of tokens as input\n",
    "    \n",
    "    returns the list of normalized tokens\n",
    "    '''\n",
    "    tokens = remove_non_ascii(tokens)\n",
    "    tokens = to_lowercase(tokens)\n",
    "    tokens = split_on_characterset(tokens, r'[/\\\\~_-]')\n",
    "    tokens = replace_numbers(tokens)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = remove_stopwords(tokens, STOP_WORDS, STOP_WORDS_EXCEPTIONS)\n",
    "    tokens = lemmatize(tokens, WordNetLemmatizer(), POS_TAG_MAP)\n",
    "    return tokens\n",
    "\n",
    "def preprocess(document):\n",
    "    '''\n",
    "    preprocesses and tokenizes the document\n",
    "    normalizes the document's tokens\n",
    "    and finally joins the normalized tokens of a document\n",
    "    to prepare it for vectorization\n",
    "    \n",
    "    returns a preprocessed document, ready for vectorization\n",
    "    '''\n",
    "    \n",
    "    document = replace_contractions(document)\n",
    "    document = remove_urls(document)\n",
    "    document = clean_sentences(document)\n",
    "    tokens = word_tokenize(document)\n",
    "    tokens = normalize(tokens)\n",
    "    document = ''.join([\" \" + token for token in tokens]).strip() \n",
    "    return document\n",
    "\n",
    "def preprocess_dataset(corpus):\n",
    "    '''\n",
    "    preprocesses all documents in a corpus designating a dataset\n",
    "    returns a corpus with preprocessed documents\n",
    "    and ready for vectorization\n",
    "    '''\n",
    "    return [preprocess(document) for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    read two three negative review main page imdb ...\n",
       "1    recently may two thousand and eight discover c...\n",
       "2    pathetic word bad act pathetic script cheezy d...\n",
       "3    spencer tracy katherine hepburn would roll gra...\n",
       "4    opinion one best action movie 1970s not featur...\n",
       "Name: Avis, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PREPROCESSING DATASET\n",
    "df_transformed = df2.copy() #creating a new copy of the dataset that will be preprocessed\n",
    "df_transformed['Avis'] = preprocess_dataset(df_transformed['Avis']) #preprocessing of opinions column\n",
    "display(df_transformed['Avis'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VECTORIZATION\n",
    "\n",
    "#Splitting the dataset prior to vectorization, to prevent memory related errors during processing\n",
    "df_transformed1 = df_transformed.iloc[:5000]\n",
    "\n",
    "#vectorization of the opinions column\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df_transformed1['Avis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION USING ACCURACY METRIC\n",
    "\n",
    "#choosing the data (opinions) and target (score) columns in the dataset\n",
    "X = vectors.toarray()\n",
    "y = df_transformed1['Score']\n",
    "\n",
    "#dictionary containing the models to cross validate using their default parameters\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'SGDClassifier': SGDClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'LinearSVC': LinearSVC()\n",
    "}\n",
    "\n",
    "#configuring the parameters used by the cross validation function\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=CV_SEED)\n",
    "\n",
    "#cross validation using accuracy metric\n",
    "#for each defined model\n",
    "for name, model in models.items():\n",
    "    start_time = time()\n",
    "    print('Cross validation started at {}'.format(datetime.now()))\n",
    "    cv_score = cross_val_score(model, X, y, cv=k_fold, scoring=CV_SCORING)\n",
    "    output = \"\"\"\n",
    "    Time taken to complete cross validation of {}: {} seconds\n",
    "    Accuracy scores over 10 evaluations: {}\n",
    "    Mean score: {}\n",
    "    Standard deviation of scores: {}\n",
    "    \"\"\".format(name, time() - start_time, cv_score, cv_score.mean(), cv_score.std())\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTS OF THE CROSS VALIDATION ON BOTH PARTITIONS\n",
    "\n",
    "#FIRST PARTITION [0:5000]\n",
    "########################\n",
    "# Cross validation started at 2019-04-12 13:56:55.212174\n",
    "# \n",
    "#     Time taken to complete cross validation of LogisticRegression: 7.469494581222534 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.908 0.89  0.894 0.9   0.884 0.916 0.892 0.91  0.906 0.886]\n",
    "#     Mean score: 0.8986000000000001\n",
    "#     Standard deviation of scores: 0.01043264108459599\n",
    "# \n",
    "# Cross validation started at 2019-04-12 13:57:02.682150\n",
    "# \n",
    "#     Time taken to complete cross validation of SGDClassifier: 16.67750573158264 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.916 0.88  0.854 0.858 0.85  0.916 0.812 0.912 0.908 0.812]\n",
    "#     Mean score: 0.8718\n",
    "#     Standard deviation of scores: 0.038775765627515335\n",
    "#\n",
    "# Cross validation started at 2019-04-12 14:07:25.906064\n",
    "# \n",
    "#     Time taken to complete cross validation of DecisionTreeClassifier: 443.644727230072 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.758 0.722 0.754 0.728 0.734 0.78  0.726 0.742 0.736 0.75 ]\n",
    "#     Mean score: 0.743\n",
    "#     Standard deviation of scores: 0.016881943016134146\n",
    "#\n",
    "# Cross validation started at 2019-04-12 14:14:49.551167\n",
    "# \n",
    "#     Time taken to complete cross validation of RandomForestClassifier: 44.67994499206543 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.77  0.778 0.774 0.766 0.79  0.784 0.804 0.764 0.78  0.772]\n",
    "#     Mean score: 0.7782000000000001\n",
    "#     Standard deviation of scores: 0.011469960767151744\n",
    "# \n",
    "# Cross validation started at 2019-04-12 14:00:25.789652\n",
    "# \n",
    "#     Time taken to complete cross validation of GaussianNB: 28.69598889350891 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.676 0.642 0.69  0.68  0.638 0.7   0.66  0.682 0.68  0.672]\n",
    "#     Mean score: 0.6719999999999999\n",
    "#     Standard deviation of scores: 0.018846750383023584\n",
    "# \n",
    "# Cross validation started at 2019-04-12 14:35:25.204313\n",
    "# \n",
    "#     Time taken to complete cross validation of KNeighborsClassifier: 1241.7990498542786 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.794 0.782 0.782 0.804 0.764 0.788 0.806 0.824 0.776 0.796]\n",
    "#     Mean score: 0.7916\n",
    "#     Standard deviation of scores: 0.01624315240339756\n",
    "# \n",
    "# Cross validation started at 2019-04-12 14:56:07.003706\n",
    "# \n",
    "#     Time taken to complete cross validation of LinearSVC: 7.24153470993042 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.916 0.888 0.902 0.91  0.902 0.932 0.904 0.91  0.912 0.902]\n",
    "#     Mean score: 0.9077999999999999\n",
    "#     Standard deviation of scores: 0.010897706180660232\n",
    "\n",
    "#SECOND PARTITION [5000:10000]\n",
    "##############################\n",
    "# Cross validation started at 2019-04-12 13:58:46.938275\n",
    "# \n",
    "#     Time taken to complete cross validation of LogisticRegression: 7.57363748550415 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.902 0.912 0.894 0.912 0.894 0.89  0.904 0.912 0.908 0.892]\n",
    "#     Mean score: 0.9019999999999999\n",
    "#     Standard deviation of scores: 0.008438009243891603\n",
    "# \n",
    "# Cross validation started at 2019-04-12 13:58:54.512393\n",
    "# \n",
    "#     Time taken to complete cross validation of SGDClassifier: 16.909173488616943 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.924 0.91  0.848 0.882 0.89  0.9   0.882 0.91  0.922 0.9  ]\n",
    "#     Mean score: 0.8968\n",
    "#     Standard deviation of scores: 0.021469979040511445\n",
    "#\n",
    "# Cross validation started at 2019-04-12 14:18:27.835834\n",
    "# \n",
    "#     Time taken to complete cross validation of DecisionTreeClassifier: 453.8783338069916 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.706 0.712 0.696 0.77  0.718 0.74  0.74  0.76  0.706 0.736]\n",
    "#     Mean score: 0.7283999999999999\n",
    "#     Standard deviation of scores: 0.02342306555513178\n",
    "# \n",
    "# Cross validation started at 2019-04-12 14:26:01.714540\n",
    "# \n",
    "#     Time taken to complete cross validation of RandomForestClassifier: 46.442110776901245 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.75  0.796 0.8   0.764 0.764 0.756 0.772 0.776 0.808 0.738]\n",
    "#     Mean score: 0.7724\n",
    "#     Standard deviation of scores: 0.021666564102321366\n",
    "#\n",
    "# Cross validation started at 2019-04-12 14:04:24.184877\n",
    "# \n",
    "#     Time taken to complete cross validation of GaussianNB: 29.050445556640625 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.68  0.684 0.648 0.698 0.688 0.67  0.704 0.662 0.664 0.664]\n",
    "#     Mean score: 0.6761999999999999\n",
    "#     Standard deviation of scores: 0.016720047846821465\n",
    "#\n",
    "# Cross validation started at 2019-04-12 15:00:11.372601\n",
    "# \n",
    "#     Time taken to complete cross validation of KNeighborsClassifier: 1274.044404745102 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.784 0.78  0.776 0.82  0.754 0.774 0.772 0.782 0.81  0.768]\n",
    "#     Mean score: 0.7819999999999999\n",
    "#     Standard deviation of scores: 0.018482424083436668\n",
    "# \n",
    "# Cross validation started at 2019-04-12 15:21:25.417337\n",
    "# \n",
    "#     Time taken to complete cross validation of LinearSVC: 7.582218647003174 seconds\n",
    "#     Accuracy scores over 10 evaluations: [0.916 0.928 0.912 0.916 0.898 0.908 0.898 0.908 0.928 0.896]\n",
    "#     Mean score: 0.9108000000000003\n",
    "#     Standard deviation of scores: 0.010998181667894026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search started at 2019-04-12 15:52:13.070429\n",
      "\n",
      "Time taken to complete Grid search of LogisticRegression: 31.332935571670532 seconds\n",
      "\n",
      "        Model: LogisticRegression\n",
      "        Best Accuracy Score: 0.8666666666666667\n",
      "        Best Estimator: LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "        \n",
      "Grid search started at 2019-04-12 15:52:44.404189\n",
      "\n",
      "Time taken to complete Grid search of LinearSVC: 13.616746187210083 seconds\n",
      "\n",
      "        Model: LinearSVC\n",
      "        Best Accuracy Score: 0.8686666666666667\n",
      "        Best Estimator: LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "#GRIDSEARCH USING THE ACCURACY METRIC FOR PARAMETERS TUNING\n",
    "\n",
    "#based on the cross-validation results, using KFold over 10 partitions\n",
    "#the models LogisticRegression and LinearSVC are best suited for the job\n",
    "\n",
    "#dictionary containing the candidate models that will be used \n",
    "#for parameters tuning using a GridSearchCV\n",
    "candidates = {\n",
    "    'LogisticRegression': models['LogisticRegression'],\n",
    "    'LinearSVC': models['LinearSVC']\n",
    "}\n",
    "\n",
    "#dictionary of the hyperparameters to be tuned for each model\n",
    "grid_params = {\n",
    "    'LogisticRegression': [\n",
    "        {'C': np.logspace(-3,3,7)},\n",
    "        {'penalty': ['l1','l2']}\n",
    "    ],\n",
    "    'LinearSVC': [\n",
    "        {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "#class used to encapsulate the results of the gridsearch\n",
    "class GridSearchResult:\n",
    "    \n",
    "    def __init__(self, name, score, estimator):\n",
    "        self.name = name\n",
    "        self.score = score\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '''\n",
    "        Model: {}\n",
    "        Best Accuracy Score: {}\n",
    "        Best Estimator: {}\n",
    "        '''.format(self.name, self.score, self.estimator)\n",
    "\n",
    "#generation of training/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = TTS_VALIDATION_SIZE,\n",
    "    test_size = TTS_TEST_SIZE,\n",
    "    random_state = TTS_SEED\n",
    ")\n",
    "\n",
    "#GridSearchCV for every candidate classifier\n",
    "grid_search_results = []\n",
    "for name, model in candidates.items():\n",
    "    #creation of the gridsearch\n",
    "    grd_sr = GridSearchCV(\n",
    "        estimator = model,\n",
    "        param_grid = grid_params[name],\n",
    "        scoring = GRDSR_SCORING,\n",
    "        cv = 5,\n",
    "        n_jobs = -1,\n",
    "        iid = True,\n",
    "        return_train_score = True\n",
    "    )\n",
    "    \n",
    "    #execution of the gridsearch\n",
    "    start_time = time()\n",
    "    print('Grid search started at {}'.format(datetime.now()))\n",
    "    grd_sr.fit(X_train, y_train)\n",
    "    print('\\nTime taken to complete Grid search of {}: {} seconds'.format(name, time() - start_time))\n",
    "    grd_sr_result = GridSearchResult(name, grd_sr.best_score_, grd_sr.best_estimator_)\n",
    "    print(grd_sr_result)\n",
    "    grid_search_results.append(grd_sr_result)\n",
    "\n",
    "#Sorting the results by descending order on the score attribute of the GridSearchResult objects\n",
    "#to get the best candidate with the best parameters\n",
    "grid_search_results = sorted(grid_search_results, key=lambda result: result.score, reverse=True)\n",
    "clf = grid_search_results[0].estimator #best candidate with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution started at 2019-04-12 17:13:17.557336\n",
      "\n",
      "Time taken to complete pipeline execution: 15.977086544036865 seconds\n",
      "\n",
      "Prediction started at 2019-04-12 17:13:33.534615\n",
      "\n",
      "Time taken to complete prediction: 34.844505071640015 seconds\n",
      "\n",
      "Accuracy: 0.892\n",
      "Confusion Matrix\n",
      "[[1547  190]\n",
      " [ 188 1575]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.89      0.89      1737\n",
      "           1       0.89      0.89      0.89      1763\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      3500\n",
      "   macro avg       0.89      0.89      0.89      3500\n",
      "weighted avg       0.89      0.89      0.89      3500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PIPELINE CREATION\n",
    "\n",
    "#creating the pipeline instance\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=preprocess)),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "#choosing data and target columns from initial dataset\n",
    "df_pipeline = df2.iloc[:5000]\n",
    "X = df_pipeline['Avis']\n",
    "y = df_pipeline['Score']\n",
    "\n",
    "#generating the training/test sets from the initial dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = TTS_VALIDATION_SIZE,\n",
    "    test_size = TTS_TEST_SIZE,\n",
    "    random_state = TTS_SEED\n",
    ")\n",
    "\n",
    "#learning the model using the pipeline\n",
    "start_time = time()\n",
    "print('Pipeline execution started at {}'.format(datetime.now()))\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('\\nTime taken to complete pipeline execution: {} seconds'.format(time() - start_time))\n",
    "\n",
    "#predicting the targets of test data\n",
    "start_time = time()\n",
    "print('\\nPrediction started at {}'.format(datetime.now()))\n",
    "prediction_result = pipeline.predict(X_test)\n",
    "print('\\nTime taken to complete prediction: {} seconds'.format(time() - start_time))\n",
    "\n",
    "#printing the accuracy, confusion matrix and classification report\n",
    "#of the classifier in the pipeline\n",
    "accuracy = accuracy_score(prediction_result, y_test)\n",
    "conf = confusion_matrix(y_test, prediction_result)\n",
    "report = classification_report(y_test, prediction_result)\n",
    "print('''\n",
    "Accuracy: {}\n",
    "Confusion Matrix\n",
    "{}\n",
    "\n",
    "Classification Report\n",
    "{}\n",
    "'''.format(accuracy, conf, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the pipeline containing the trained model\n"
     ]
    }
   ],
   "source": [
    "#PIPELINE SAVING\n",
    "print('Saving the pipeline containing the trained model')\n",
    "pickle.dump(pipeline, open(PIPELINE_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDB Opinions Dataset\n",
      "Size : (2000, 1)\n",
      "Head of imported dataset :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Firstly, few colleges allow students to take c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For years, I've been a big fan of Park's work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...now please move on because that's getting o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was shown on a premium channel, so I didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before I start to tear apart this movie, mark ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis\n",
       "0  Firstly, few colleges allow students to take c...\n",
       "1  For years, I've been a big fan of Park's work ...\n",
       "2  ...now please move on because that's getting o...\n",
       "3  This was shown on a premium channel, so I didn...\n",
       "4  Before I start to tear apart this movie, mark ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of shuffled dataset :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It may interest people to know that this film ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of the better made for TV biopics, I just ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Writer &amp; director Jay Andrews, a.k.a. Jim Wyno...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie probably isn't the funniest I've ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bank heist / Cop thriller sounds OK right?&lt;br ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  It may interest people to know that this film ...      1\n",
       "1  One of the better made for TV biopics, I just ...      1\n",
       "2  Writer & director Jay Andrews, a.k.a. Jim Wyno...     -1\n",
       "3  This movie probably isn't the funniest I've ev...      1\n",
       "4  Bank heist / Cop thriller sounds OK right?<br ...     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of test data...\n",
      "Accuracy: 0.469\n",
      "Confusion Matrix: [[436 564]\n",
      " [498 502]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.44      0.45      1000\n",
      "           1       0.47      0.50      0.49      1000\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      2000\n",
      "   macro avg       0.47      0.47      0.47      2000\n",
      "weighted avg       0.47      0.47      0.47      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PIPELINE LOADING\n",
    "\n",
    "#Loading the pipeline containing the trained classifier\n",
    "clf_loaded = pickle.load(open(PIPELINE_PATH, 'rb'))\n",
    "\n",
    "#some test data and real targets\n",
    "data_df = import_dataset(\"imdb_reviews.csv\", 'IMDB Opinions Dataset', sep='\\t', names=['Avis'])\n",
    "test_target = []\n",
    "for i in range(2000):\n",
    "    if i < 1000:\n",
    "        test_target.append(-1)\n",
    "    else:\n",
    "        test_target.append(1)\n",
    "\n",
    "data_df['Score'] = test_target\n",
    "data_df = shuffle_dataset(data_df)\n",
    "\n",
    "#prediction of data\n",
    "print('Prediction of test data...')\n",
    "prediction_results = clf_loaded.predict(data_df['Avis'])\n",
    "# print('Comparison between real and predicted values')\n",
    "# for i in range(len(prediction_results)):\n",
    "#     print('''\n",
    "#     Avis: {}\n",
    "#     Real: {}\n",
    "#     Predicted: {}\\n\n",
    "#     '''.format(data_df['Avis'][i], test_target[i], -1))\n",
    "\n",
    "\n",
    "#comparison between real and predicted values\n",
    "print('Accuracy: {}'.format(accuracy_score(prediction_results, test_target)))\n",
    "print('Confusion Matrix: {}'.format(confusion_matrix(test_target, prediction_results)))\n",
    "print('Classification report:')\n",
    "print(classification_report(test_target, prediction_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
