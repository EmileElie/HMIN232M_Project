{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATION DES MODULES\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import inflect\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import punkt\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "##### UNCOMMENT THIS SECTION IF FIRT TIME RUNNING\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "#####\n",
    "\n",
    "# Set seed for random results base calculation\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe des avis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis\n",
       "0  Obviously made to show famous 1950s stripper M...\n",
       "1  This film was more effective in persuading me ...\n",
       "2  Unless you are already familiar with the pop s...\n",
       "3  From around the time Europe began fighting Wor...\n",
       "4  Im not surprised that even cowgirls get the bl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe des scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score\n",
       "0     -1\n",
       "1     -1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe merged\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  Obviously made to show famous 1950s stripper M...     -1\n",
       "1  This film was more effective in persuading me ...     -1\n",
       "2  Unless you are already familiar with the pop s...     -1\n",
       "3  From around the time Europe began fighting Wor...     -1\n",
       "4  Im not surprised that even cowgirls get the bl...     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#LECTURE DES FICHIERS\n",
    "print(\"\\nDataframe des avis\")\n",
    "df_avis = pd.read_csv('Dataset/dataset.csv', sep = '\\t', header = None, names = ['Avis'], encoding ='utf-8')\n",
    "display(df_avis.head())\n",
    "display(df_avis.shape)\n",
    "\n",
    "print(\"\\nDataframe des scores\")\n",
    "df_score = pd.read_csv('Dataset/labels.csv', sep = '\\t', header = None, names = ['Score'], encoding ='utf-8')\n",
    "display(df_score.head())\n",
    "display(df_score.shape)\n",
    "\n",
    "print(\"\\nDataframe merged\")\n",
    "df = df_avis.join(df_score)\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After having read two or three negative review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I recently (May 2008) discovered that this chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pathetic is the word. Bad acting, pathetic scr...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spencer Tracy and Katherine Hepburn would roll...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This in my opinion is one of the best action m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  After having read two or three negative review...      1\n",
       "1  I recently (May 2008) discovered that this chi...      1\n",
       "2  Pathetic is the word. Bad acting, pathetic scr...     -1\n",
       "3  Spencer Tracy and Katherine Hepburn would roll...     -1\n",
       "4  This in my opinion is one of the best action m...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copie du DF originale\n",
    "df2 = shuffle(df)\n",
    "# Réinitialisation des index\n",
    "#df2.reset_index()\n",
    "df2.reset_index(inplace = True, \n",
    "                drop = True)\n",
    "display(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DÉFINITION DES CONSTANTES UTILISÉES\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "STOP_WORDS_EXCEPTIONS = set(('not',))\n",
    "\n",
    "#dictionnaire des POS-Tags\n",
    "POS_TAG_MAP = defaultdict(lambda : wn.NOUN)\n",
    "POS_TAG_MAP['J'] = wn.ADJ\n",
    "POS_TAG_MAP['V'] = wn.VERB\n",
    "POS_TAG_MAP['R'] = wn.ADV\n",
    "\n",
    "#Seeds utilisé pour le CV score et la génération des training/test sets\n",
    "TTS_SEED = 30 #training_test_split seed\n",
    "CV_SEED = 7 #cross_val_score seed\n",
    "\n",
    "#Métrique utilisé pour évaluer les classifieurs lors du CV score et GridSearchCV\n",
    "SCORING = 'accuracy'\n",
    "\n",
    "#DÉFINITION DES FONCTIONS DE PRÉTRAITEMENTS\n",
    "def replace_contractions(document):\n",
    "    \"\"\"\n",
    "    replaces contracted expressions in a document\n",
    "    \n",
    "    returns document with no contracted expressions\n",
    "    \"\"\"\n",
    "    return contractions.fix(document)\n",
    "\n",
    "def remove_urls(document):\n",
    "    \"\"\"\n",
    "    removes all urls in the document\n",
    "    \n",
    "    return a document without any urls\n",
    "    \"\"\"\n",
    "    return re.sub(r'https?://(www\\.)?[-\\w@:%.\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-\\w@:%_\\+.~#?&/=;]*)', '', document)\n",
    "\n",
    "def clean_document(document):\n",
    "    word_word = r'([a-zA-Z]+\\.*)\\.([a-zA-Z]+)'\n",
    "    word_digit = r'([a-zA-Z]+\\.*)\\.(\\d+)'\n",
    "    digit_digit = r'(\\d+\\.*)\\.([a-zA-Z]+)'\n",
    "    patterns = [\n",
    "        word_word, #word.word pattern\n",
    "        word_digit, #word.digit pattern\n",
    "        digit_digit, #digit.word pattern\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, document):\n",
    "            document = re.sub(pattern, r'\\1. \\2', document)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def remove_non_ascii(tokens):\n",
    "    '''\n",
    "    normalizes the tokens\n",
    "    encodes tokens as ASCII characters from tokens\n",
    "    and decodes as utf-8\n",
    "    \n",
    "    returns a list of normalized and encoded as ascii tokens\n",
    "    '''\n",
    "    return [unicodedata.normalize('NFKD', token)\n",
    "           .encode('ascii', 'ignore')\n",
    "           .decode('utf-8', 'ignore')\n",
    "           for token in tokens]\n",
    "\n",
    "def split_on_characterset(tokens, regex):\n",
    "    '''\n",
    "    splits a token in tokens upon matching with the characterset defined by the regex\n",
    "    appends the tokens obtained from splitting the token to the tokens list\n",
    "    \n",
    "    returns a list of all tokens obtained after splitting problematic tokens\n",
    "    '''\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search(regex, token) :\n",
    "            new_tokens += re.split(regex, token)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"returns a list of tokens in lowercase\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def replace_numbers(tokens):\n",
    "    \"\"\"\n",
    "    replaces tokens representing whole numeric values\n",
    "    by their equivalent letter values\n",
    "    \n",
    "    returns a list of transformed tokens\n",
    "    \"\"\"\n",
    "    engine = inflect.engine()\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = token\n",
    "        if token.isdigit():\n",
    "            new_token = engine.number_to_words(token)\n",
    "        new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    \"\"\"\n",
    "    removes tokens not in \\w and \\s classes of characters.\n",
    "    by extension, all punctuation characters will be removed\n",
    "    \n",
    "    returns a list of tokens only in \\w and \\s\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        if new_token != '':\n",
    "            new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_stopwords(tokens, stopwords, exceptions):\n",
    "    '''\n",
    "    removes all stopwords (a set) from tokens (a list)\n",
    "    except those in exceptions (a set)\n",
    "    \n",
    "    returns a list of tokens that are not stopwords\n",
    "    '''\n",
    "    stop = stopwords - exceptions\n",
    "    return [token for token in tokens if token not in stop]\n",
    "\n",
    "def lemmatize(tokens, lemmatizer, pos_tag_map):\n",
    "    '''\n",
    "    lematizes all tokens using a lemmatizer and a POS-Tagging map\n",
    "    \n",
    "    returns the list of lemmatized tokens\n",
    "    '''\n",
    "    return [lemmatizer.lemmatize(token, pos_tag_map[tag[0]]) for token, tag in pos_tag(tokens)]\n",
    "    \n",
    "def normalize(tokens):\n",
    "    '''\n",
    "    normalizes all the tokens by using all preprocessing\n",
    "    functions taking a list of tokens as input\n",
    "    \n",
    "    returns the list of normalized tokens\n",
    "    '''\n",
    "    tokens = remove_non_ascii(tokens)\n",
    "    tokens = to_lowercase(tokens)\n",
    "    tokens = split_on_characterset(tokens, r'[/\\\\~_-]')\n",
    "    tokens = replace_numbers(tokens)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = remove_stopwords(tokens, STOP_WORDS, STOP_WORDS_EXCEPTIONS)\n",
    "    tokens = lemmatize(tokens, WordNetLemmatizer(), POS_TAG_MAP)\n",
    "    return tokens\n",
    "\n",
    "def preprocess(document):\n",
    "    document = replace_contractions(document) #remplacement des contractions dans le document\n",
    "    document = remove_urls(document) #supprimer les urls dans le document\n",
    "    document = clean_document(document)\n",
    "    tokens = word_tokenize(document) #list des tokens du document\n",
    "    tokens = normalize(tokens) #list des tokens normalisés du document\n",
    "    document = ''.join([\" \" + token for token in tokens]).strip() #rejoindre les tokens normalisés pour obtenir le document nettoyé \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    read two three negative review main page imdb ...\n",
       "1    recently may two thousand and eight discover c...\n",
       "2    pathetic word bad act pathetic script cheezy d...\n",
       "3    spencer tracy katherine hepburn would roll gra...\n",
       "4    opinion one best action movie 1970s not featur...\n",
       "Name: Avis, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocessing dataset\n",
    "df_transformed = df2.copy()\n",
    "df_transformed['Avis'] = [preprocess(document) for document in df_transformed['Avis']]\n",
    "display(df_transformed['Avis'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#splitting dataset\n",
    "df_transformed1 = df_transformed.iloc[:4000]\n",
    "\n",
    "#Vectorizing\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df_transformed1['Avis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Temps pris pour la cross validation de KNeighborsClassifier : 653.8217942714691 secondes\n",
      "    Accuracy scores pour les 10 évaluations : [0.785  0.77   0.8025 0.7575 0.79   0.765  0.7875 0.7975 0.7925 0.7725]\n",
      "    Score moyen : 0.7820000000000001\n",
      "    Écart type des scores : 0.014133294025102577\n",
      "    \n",
      "\n",
      "    Temps pris pour la cross validation de SVC : 4856.297691583633 secondes\n",
      "    Accuracy scores pour les 10 évaluations : [0.495  0.4925 0.4925 0.4875 0.45   0.47   0.485  0.4575 0.5    0.47  ]\n",
      "    Score moyen : 0.48\n",
      "    Écart type des scores : 0.016201851746019645\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Choix des features et des classes\n",
    "X = vectors.toarray()\n",
    "y = df_transformed1['Score']\n",
    "\n",
    "#Création du dictionnaire contenant les classifieurs et leurs paramètres par défaut\n",
    "models = {\n",
    "    #'LogisticRegression': LogisticRegression(),\n",
    "    #'SGDClassifier': SGDClassifier(),\n",
    "    #'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'RandomForestClassifier', RandomForestClassifier(),\n",
    "    #'GaussianNB': GaussianNB(),\n",
    "    #'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    #'SVC': SVC(),\n",
    "    'LinearSVC': LinearSVC()\n",
    "}\n",
    "\n",
    "#configuration des paramètres utilisés par la cross validation\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=CV_SEED)\n",
    "\n",
    "#cross validation sur l'ensemble des classifieurs choisis\n",
    "#en utilisant la mesure accuracy\n",
    "for name, model in models.items():\n",
    "    print('Cross validation a commencé à {}'.format(datetime.now()))\n",
    "    start_time = time()\n",
    "    cv_score = cross_val_score(model, X, y, cv=k_fold, scoring=SCORING)\n",
    "    output = \"\"\"\n",
    "    Temps pris pour la cross validation de {} : {} secondes\n",
    "    Accuracy scores pour les 10 évaluations : {}\n",
    "    Score moyen : {}\n",
    "    Écart type des scores : {}\n",
    "    \"\"\".format(name, time() - start_time, cv_score, cv_score.mean(), cv_score.std())\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.3\n",
    "test_size = 1 - validation_size\n",
    "seed = 30\n",
    "\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = validation_size,\n",
    "    test_size = test_size,\n",
    "    random_state = seed\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
