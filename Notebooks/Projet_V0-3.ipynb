{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATION DES MODULES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import punkt\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "##### UNCOMMENT THIS SECTION IF FIRT TIME RUNNING\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "#####\n",
    "\n",
    "# Set seed for random results base calculation\n",
    "np.random.seed(500)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe des avis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis\n",
       "0  Obviously made to show famous 1950s stripper M...\n",
       "1  This film was more effective in persuading me ...\n",
       "2  Unless you are already familiar with the pop s...\n",
       "3  From around the time Europe began fighting Wor...\n",
       "4  Im not surprised that even cowgirls get the bl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe des scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score\n",
       "0     -1\n",
       "1     -1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe merged\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obviously made to show famous 1950s stripper M...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film was more effective in persuading me ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unless you are already familiar with the pop s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From around the time Europe began fighting Wor...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im not surprised that even cowgirls get the bl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  Obviously made to show famous 1950s stripper M...     -1\n",
       "1  This film was more effective in persuading me ...     -1\n",
       "2  Unless you are already familiar with the pop s...     -1\n",
       "3  From around the time Europe began fighting Wor...     -1\n",
       "4  Im not surprised that even cowgirls get the bl...     -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#LECTURE DES FICHIERS\n",
    "print(\"\\nDataframe des avis\")\n",
    "df_avis = pd.read_csv('Dataset/dataset.csv', sep = '\\t', header = None, names = ['Avis'], encoding ='utf-8')\n",
    "display(df_avis.head())\n",
    "display(df_avis.shape)\n",
    "\n",
    "print(\"\\nDataframe des scores\")\n",
    "df_score = pd.read_csv('Dataset/labels.csv', sep = '\\t', header = None, names = ['Score'], encoding ='utf-8')\n",
    "display(df_score.head())\n",
    "display(df_score.shape)\n",
    "\n",
    "print(\"\\nDataframe merged\")\n",
    "df = df_avis.join(df_score)\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It makes sense to me that this film is getting...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kate Beckinsale steals the show! Bravo! Too ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is probably one of the most original love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw the new redubbed and edited version yest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had seen this film way back in the 80's and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Avis  Score\n",
       "0  It makes sense to me that this film is getting...     -1\n",
       "1  Kate Beckinsale steals the show! Bravo! Too ba...      1\n",
       "2  This is probably one of the most original love...      1\n",
       "3  I saw the new redubbed and edited version yest...      1\n",
       "4  I had seen this film way back in the 80's and ...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copie du DF originale\n",
    "df2 = shuffle(df)\n",
    "# Réinitialisation des index\n",
    "#df2.reset_index()\n",
    "df2.reset_index(inplace = True, \n",
    "                drop = True)\n",
    "display(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DÉFINITION DES CONSTANTES UTILISÉES\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "STOP_WORDS_EXCEPTIONS = set(('not',))\n",
    "\n",
    "#dictionnaire des POS-Tags\n",
    "POS_TAG_MAP = defaultdict(lambda : wn.NOUN)\n",
    "POS_TAG_MAP['J'] = wn.ADJ\n",
    "POS_TAG_MAP['V'] = wn.VERB\n",
    "POS_TAG_MAP['R'] = wn.ADV\n",
    "\n",
    "#DÉFINITION DES FONCTIONS DE PRÉTRAITEMENTS\n",
    "def replace_contractions(document):\n",
    "    \"\"\"\n",
    "    replaces contracted expressions in a document\n",
    "    \n",
    "    returns document with no contracted expressions\n",
    "    \"\"\"\n",
    "    return contractions.fix(document)\n",
    "\n",
    "def remove_urls(document):\n",
    "    \"\"\"\n",
    "    removes all urls in the document\n",
    "    \n",
    "    return a document without any urls\n",
    "    \"\"\"\n",
    "    return re.sub(r'https?://(www\\.)?[-\\w@:%.\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-\\w@:%_\\+.~#?&/=;]*)', '', document)\n",
    "\n",
    "def clean_document(document):\n",
    "    word_word = r'([a-zA-Z]+\\.*)\\.([a-zA-Z]+)'\n",
    "    word_digit = r'([a-zA-Z]+\\.*)\\.(\\d+)'\n",
    "    digit_digit = r'(\\d+\\.*)\\.([a-zA-Z]+)'\n",
    "    patterns = [\n",
    "        word_word, #word.word pattern\n",
    "        word_digit, #word.digit pattern\n",
    "        digit_digit, #digit.word pattern\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, document):\n",
    "            document = re.sub(pattern, r'\\1. \\2', document)\n",
    "    \n",
    "    return document\n",
    "\n",
    "def remove_non_ascii(tokens):\n",
    "    '''\n",
    "    normalizes the tokens\n",
    "    encodes tokens as ASCII characters from tokens\n",
    "    and decodes as utf-8\n",
    "    \n",
    "    returns a list of normalized and encoded as ascii tokens\n",
    "    '''\n",
    "    return [unicodedata.normalize('NFKD', token)\n",
    "           .encode('ascii', 'ignore')\n",
    "           .decode('utf-8', 'ignore')\n",
    "           for token in tokens]\n",
    "\n",
    "def split_on_characterset(tokens, regex):\n",
    "    '''\n",
    "    splits a token in tokens upon matching with the characterset defined by the regex\n",
    "    appends the tokens obtained from splitting the token to the tokens list\n",
    "    \n",
    "    returns a list of all tokens obtained after splitting problematic tokens\n",
    "    '''\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search(regex, token) :\n",
    "            new_tokens += re.split(regex, token)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"returns a list of tokens in lowercase\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def replace_numbers(tokens):\n",
    "    \"\"\"\n",
    "    replaces tokens representing whole numeric values\n",
    "    by their equivalent letter values\n",
    "    \n",
    "    returns a list of transformed tokens\n",
    "    \"\"\"\n",
    "    engine = inflect.engine()\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = token\n",
    "        if token.isdigit():\n",
    "            new_token = engine.number_to_words(token)\n",
    "        new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    \"\"\"\n",
    "    removes tokens not in \\w and \\s classes of characters.\n",
    "    by extension, all punctuation characters will be removed\n",
    "    \n",
    "    returns a list of tokens only in \\w and \\s\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        if new_token != '':\n",
    "            new_tokens.append(new_token)\n",
    "    return new_tokens\n",
    "\n",
    "def remove_stopwords(tokens, stopwords, exceptions):\n",
    "    '''\n",
    "    removes all stopwords (a set) from tokens (a list)\n",
    "    except those in exceptions (a set)\n",
    "    \n",
    "    returns a list of tokens that are not stopwords\n",
    "    '''\n",
    "    stop = stopwords - exceptions\n",
    "    return [token for token in tokens if token not in stop]\n",
    "\n",
    "def lemmatize(tokens, lemmatizer, pos_tag_map):\n",
    "    '''\n",
    "    lematizes all tokens using a lemmatizer and a POS-Tagging map\n",
    "    \n",
    "    returns the list of lemmatized tokens\n",
    "    '''\n",
    "    return [lemmatizer.lemmatize(token, pos_tag_map[tag[0]]) for token, tag in pos_tag(tokens)]\n",
    "    \n",
    "def normalize(tokens):\n",
    "    '''\n",
    "    normalizes all the tokens by using all preprocessing\n",
    "    functions taking a list of tokens as input\n",
    "    \n",
    "    returns the list of normalized tokens\n",
    "    '''\n",
    "    tokens = remove_non_ascii(tokens)\n",
    "    tokens = to_lowercase(tokens)\n",
    "    tokens = split_on_characterset(tokens, r'[/\\\\~_-]')\n",
    "    tokens = replace_numbers(tokens)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = remove_stopwords(tokens, STOP_WORDS, STOP_WORDS_EXCEPTIONS)\n",
    "    tokens = lemmatize(tokens, WordNetLemmatizer(), POS_TAG_MAP)\n",
    "    return tokens\n",
    "\n",
    "def preprocess(document):\n",
    "    document = replace_contractions(document) #remplacement des contractions dans le document\n",
    "    document = remove_urls(document) #supprimer les urls dans le document\n",
    "    document = clean_document(document)\n",
    "    tokens = word_tokenize(document) #list des tokens du document\n",
    "    tokens = normalize(tokens) #list des tokens normalisés du document\n",
    "    document = ''.join([\" \" + token for token in tokens]).strip() #rejoindre les tokens normalisés pour obtenir le document nettoyé \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    make sense film get raf hollywood oftentimes h...\n",
       "1    kate beckinsale steal show bravo bad knightly ...\n",
       "2    probably one original love story see age espec...\n",
       "3    saw new redubbed edit version yesterday love g...\n",
       "4    see film way back eighty nearly forgotten noti...\n",
       "Name: Avis, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocessing dataset\n",
    "df_transformed = df2.copy()\n",
    "df_transformed['Avis'] = [preprocess(document) for document in df_transformed['Avis']]\n",
    "display(df_transformed['Avis'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#splitting dataset\n",
    "df_transformed1 = df_transformed.iloc[0:6500]\n",
    "\n",
    "#Vectorizing\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df_transformed1['Avis'])\n",
    "df_vectorized = pd.DataFrame(data=vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "# display(df_vectorized.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_transformed['Score']\n",
    "validation_size = 0.3\n",
    "test_size = 1 - validation_size\n",
    "seed = 30\n",
    "\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = validation_size,\n",
    "    test_size = test_size,\n",
    "    random_state = seed\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
